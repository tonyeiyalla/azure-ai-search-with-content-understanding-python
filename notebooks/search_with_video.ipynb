{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Search with Azure Content Understanding\n",
    "## Objective\n",
    "This document is meant to present a guideline on how to leverage the Azure Video Content Understanding API for AI Search.\n",
    "The sample will demonstrate the following steps:\n",
    "1. Process a video file from Azure Blob storage with the Azure Video Content Understanding service to generate a video description grounding document.\n",
    "2. Process the video description grounding document with Azure Search client to generate an Azure Search index.\n",
    "3. Utilize OpenAI completion and embedding models to search through content in the video search index.\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "1. Follow [README](../README.md#configure-azure-ai-service-resource) to create essential resource that will be used in this sample.\n",
    "1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\", \"2024-08-01-preview\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\", \"2023-05-15\")\n",
    "\n",
    "# Load and validate Azure Search Services configs\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"sample-index-video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "VIDEO_LOCATION = Path(\"../data/video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(\n",
    "    str(parent_dir)\n",
    ")  # add the parent directory to the path to use shared modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Video Segment Description\n",
    "Create a custom analyzer with pre-defined schema. The custom analyzer schema is defined in [./video_content_understanding_basic.json](./video_content_understanding_basic.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_content_understanding.json\"\n",
    "ANALYZER_ID = \"video_analyzer\" + \"_\" + str(\n",
    "    uuid.uuid4())  # Unique identifier for the analyzer\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/search_with_video\",\n",
    ")\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(\n",
    "    ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video segment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)\n",
    "\n",
    "# Wait for the analysis to complete and get the content analysis result\n",
    "video_cu_result = cu_client.poll_result(response,\n",
    "                                        timeout_seconds=3600)  # 1 hour timeout\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result)\n",
    "\n",
    "# Delete the analyzer if it is no longer needed\n",
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the video segmention descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values_to_strings(json_obj):\n",
    "    return [str(value) for value in json_obj]\n",
    "\n",
    "\n",
    "def remove_markdown(json_obj):\n",
    "    for segment in json_obj:\n",
    "        if 'markdown' in segment:\n",
    "            del segment['markdown']\n",
    "    return json_obj\n",
    "\n",
    "\n",
    "def process_cu_scene_description(scene_description):\n",
    "    audio_visual_segments = scene_description[\"result\"][\"contents\"]\n",
    "    filtered_audio_visual_segments = remove_markdown(audio_visual_segments)\n",
    "    audio_visual_splits = [\n",
    "        \"The following is a json string representing a video segment with scene description and transcript ```\"\n",
    "        + v\n",
    "        + \"```\"\n",
    "        for v in convert_values_to_strings(filtered_audio_visual_segments)\n",
    "    ]\n",
    "    docs = [Document(page_content=v) for v in audio_visual_splits]\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = process_cu_scene_description(video_cu_result)\n",
    "print(\"There are \" + str(len(docs)) + \" documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and index the chunks\n",
    "Add the scene description segments as documents to Azure Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_index_chunks(docs):\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "        openai_api_version=AZURE_OPENAI_EMBEDDING_API_VERSION,  # e.g., \"2023-12-01-preview\"\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "        azure_search_key=None,\n",
    "        index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "        embedding_function=aoai_embeddings.embed_query\n",
    "    )\n",
    "    vector_store.add_documents(documents=docs)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# embed and index the docs:\n",
    "vector_store = embed_and_index_chunks(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant content\n",
    "#### Execute a pure vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your query\n",
    "query = \"japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search\n",
    "docs = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute hybrid search. Vector and nonvector text fields are queried in parallel, results are merged, and top matches of the unified result set are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a hybrid search using the search_type parameter\n",
    "docs = vector_store.hybrid_search(query=query, k=3)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Q&A\n",
    "We can utilize OpenAI GPT completion models + Azure Search to conversationally search for and chat about the results. (If you are using GitHub Codespaces, there will be an input prompt near the top of the screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rag chain\n",
    "prompt_str = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "def setup_rag_chain(vector_store):\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", k=3)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "    llm = AzureChatOpenAI(\n",
    "        openai_api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "        azure_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        azure_ad_token_provider=token_provider,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# Setup conversational search\n",
    "def conversational_search(rag_chain, query):\n",
    "    print(rag_chain.invoke(query))\n",
    "\n",
    "\n",
    "rag_chain = setup_rag_chain(vector_store)\n",
    "while True:\n",
    "    query = input(\"Enter your query: \")\n",
    "    if query==\"\":\n",
    "        break\n",
    "    conversational_search(rag_chain, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
